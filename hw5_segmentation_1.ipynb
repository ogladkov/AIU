{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\sm00th\\AppData\\Local\\conda\\conda\\envs\\tensorflow_gpuenv\\lib\\site-packages\\classification_models\\resnext\\__init__.py:4: UserWarning: Current ResNext models are deprecated, use keras.applications ResNeXt models\n",
      "  warnings.warn('Current ResNext models are deprecated, '\n"
     ]
    }
   ],
   "source": [
    "from segmentation_models import Unet\n",
    "from segmentation_models.backbones import get_preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "K.set_image_data_format('channels_last')\n",
    "from tqdm import tqdm\n",
    "K.clear_session()\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'resnet34'\n",
    "preprocess_input = get_preprocessing(BACKBONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drive.mount('/content/drive')\n",
    "\n",
    "# !unzip drive/My\\ Drive/Colab\\ Notebooks/hw5/dataset1.zip\n",
    "# ! mv dataset1 drive/My\\ Drive/Colab\\ Notebooks/hw5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_WIDTH, IM_HEIGHT = 360, 480\n",
    "RESOLUT = (IM_WIDTH, IM_HEIGHT)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# TR_DIR = r'/drive/My\\ Drive/Colab\\ Notebooks/hw5/dataset1/annotations_prepped_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls drive/My\\ Drive/Colab\\ Notebooks/hw5/dataset1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_prepped_train = r'C:\\Users\\sm00th\\Dropbox\\Python\\Neural Univercity\\hw5\\dataset1\\images_prepped_train'\n",
    "images_prepped_test = r'C:\\Users\\sm00th\\Dropbox\\Python\\Neural Univercity\\hw5\\dataset1\\images_prepped_test'\n",
    "annotations_prepped_train = r'C:\\Users\\sm00th\\Dropbox\\Python\\Neural Univercity\\hw5\\dataset1\\annotations_prepped_train'\n",
    "annotations_prepped_test = r'C:\\Users\\sm00th\\Dropbox\\Python\\Neural Univercity\\hw5\\dataset1\\annotations_prepped_test'\n",
    "\n",
    "# images_prepped_train = r'drive/My Drive/Colab Notebooks/hw5/dataset1/images_prepped_train'\n",
    "# annotations_prepped_train = r'drive/My Drive/Colab Notebooks/hw5/dataset1/annotations_prepped_train'\n",
    "# annotations_prepped_train_list = os.listdir(annotations_prepped_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение и обработка входных изображений\n",
    "def read_ims(path):\n",
    "    ims = []\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        ims.append(plt.imread(os.path.join(path, file)))\n",
    "    ims = preprocess_input(ims)\n",
    "    ims = np.array(ims)\n",
    "    return ims\n",
    "\n",
    "\n",
    "# Чтение и обработка лейблов\n",
    "def read_labels(path):\n",
    "    files = os.listdir(path)   \n",
    "    im = np.empty((len(files), 360, 480, 6))\n",
    "    for n, file in enumerate(files):\n",
    "        i = plt.imread(os.path.join(path, file)) * 255\n",
    "        i = i.astype('int')\n",
    "        i = np.where(i >=5, 5, i)\n",
    "        im[n] = to_categorical(i)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка в память датасетов\n",
    "train_ims = read_ims(os.path.join(images_prepped_train, 'all'))\n",
    "train_labels = read_labels(os.path.join(annotations_prepped_train, 'all'))\n",
    "test_ims = read_ims(os.path.join(images_prepped_test, 'all'))\n",
    "test_labels = read_labels(os.path.join(annotations_prepped_test, 'all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((367, 360, 480, 3),\n",
       " (367, 360, 480, 6),\n",
       " (101, 360, 480, 3),\n",
       " (101, 360, 480, 6))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка размеров\n",
    "train_ims.shape, train_labels.shape, test_ims.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание батч-генераторов\n",
    "datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = datagen.flow(train_ims, train_labels,\n",
    "                        batch_size=BATCH_SIZE)\n",
    "\n",
    "test_generator = datagen.flow(test_ims, test_labels,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 360, 480, 3), (8, 360, 480, 6))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка размеров генерации батчей\n",
    "train_generator.next()[0].shape, test_generator.next()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загруженная из интернета UNet с кроппингом\n",
    "class UNet():\n",
    "    def __init__(self):\n",
    "        print ('build UNet ...')\n",
    "\n",
    "    def get_crop_shape(self, target, refer):\n",
    "        # width, the 3rd dimension\n",
    "        cw = (target.get_shape()[2] - refer.get_shape()[2]).value\n",
    "        assert (cw >= 0)\n",
    "        if cw % 2 != 0:\n",
    "            cw1, cw2 = int(cw/2), int(cw/2) + 1\n",
    "        else:\n",
    "            cw1, cw2 = int(cw/2), int(cw/2)\n",
    "\n",
    "        # height, the 2nd dimension\n",
    "        ch = (target.get_shape()[1] - refer.get_shape()[1]).value\n",
    "        assert (ch >= 0)\n",
    "        if ch % 2 != 0:\n",
    "            ch1, ch2 = int(ch/2), int(ch/2) + 1\n",
    "        else:\n",
    "            ch1, ch2 = int(ch/2), int(ch/2)\n",
    "\n",
    "        return (ch1, ch2), (cw1, cw2)\n",
    "\n",
    "\n",
    "\n",
    "    def create_model(self, img_shape, num_class):\n",
    "        concat_axis = 3\n",
    "        inputs = layers.Input(shape = img_shape)\n",
    "\n",
    "        conv1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1_1')(inputs)\n",
    "        conv1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "\n",
    "        pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "        conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "        conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "\n",
    "        pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "        conv3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "        conv3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "\n",
    "        pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "        conv4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "        conv4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "\n",
    "        pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "        conv5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "        conv5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "        up_conv5 = layers.UpSampling2D(size=(2, 2))(conv5)\n",
    "\n",
    "        ch, cw = self.get_crop_shape(conv4, up_conv5)\n",
    "\n",
    "        crop_conv4 = layers.Cropping2D(cropping=(ch,cw))(conv4)\n",
    "\n",
    "        up6 = layers.concatenate([up_conv5, crop_conv4], axis=concat_axis)\n",
    "\n",
    "        conv6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "        conv6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "        up_conv6 = layers.UpSampling2D(size=(2, 2))(conv6)\n",
    "\n",
    "        ch, cw = self.get_crop_shape(conv3, up_conv6)\n",
    "\n",
    "        crop_conv3 = layers.Cropping2D(cropping=(ch,cw))(conv3)\n",
    "\n",
    "        up7 = layers.concatenate([up_conv6, crop_conv3], axis=concat_axis) \n",
    "\n",
    "        conv7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "        conv7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "        up_conv7 = layers.UpSampling2D(size=(2, 2))(conv7)\n",
    "\n",
    "        ch, cw = self.get_crop_shape(conv2, up_conv7)\n",
    "\n",
    "        crop_conv2 = layers.Cropping2D(cropping=(ch,cw))(conv2)\n",
    "\n",
    "        up8 = layers.concatenate([up_conv7, crop_conv2], axis=concat_axis)\n",
    "\n",
    "        conv8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "        conv8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "        up_conv8 = layers.UpSampling2D(size=(2, 2))(conv8)\n",
    "\n",
    "        ch, cw = self.get_crop_shape(conv1, up_conv8)\n",
    "\n",
    "        crop_conv1 = layers.Cropping2D(cropping=(ch,cw))(conv1)\n",
    "\n",
    "        up9 = layers.concatenate([up_conv8, crop_conv1], axis=concat_axis)\n",
    "\n",
    "        conv9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "        conv9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "        ch, cw = self.get_crop_shape(inputs, conv9)\n",
    "\n",
    "        conv9 = layers.ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(conv9)\n",
    "\n",
    "        conv10 = layers.Conv2D(num_class, (1, 1))(conv9)\n",
    "\n",
    "        model = models.Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## 'УДАЧНЫЙ ЭКСПЕРИМЕНТ' ###################\n",
    "\n",
    "# init_colors = [[np.random.randint(0, 256) for _ in range(3)] for _ in range(6)]\n",
    "\n",
    "# def target_preproc(im):\n",
    "#     im[:, :] = np.where(im[:, :] >= 5, 6, im[:, :])\n",
    "    \n",
    "#     for cl in range(6):\n",
    "#         for ch in range(im.shape[2]):\n",
    "#             im[:, :, ch] = np.where(im[:, :, ch]==cl, init_colors[cl][ch], im[:, :, ch])\n",
    "            \n",
    "#     im = np.clip(im, 0, 255) / 255\n",
    "#     return im\n",
    "\n",
    "\n",
    "# def ohe_classes(im):\n",
    "#     im = im[:, :, 0]\n",
    "#     return im\n",
    "\n",
    "# for k in range(8):\n",
    "#     plt.imshow(a[k])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация модели\n",
    "def init_model():\n",
    "    \n",
    "    ############ Модель от Павла - проблема с кропом.\n",
    "#     model = Unet('resnet34', \n",
    "#                      input_shape=(IM_WIDTH, IM_HEIGHT, 3), \n",
    "#                      intencoder_weights='imagenet', \n",
    "#                      classes=6, activation='softmax')\n",
    "    \n",
    "    ############ Модель из нета с кропом\n",
    "    unet = UNet()\n",
    "    unet = unet.create_model(img_shape=(IM_WIDTH, IM_HEIGHT, 3), \n",
    "                  num_class=6)\n",
    "\n",
    "    return unet\n",
    "\n",
    "# Компиляция и запуск фит-генератора\n",
    "def train_model(model, epochs=20):\n",
    "    model.compile('Adam', \n",
    "#                  loss='categorical_crossentropy',\n",
    "                 loss='bce_jaccard_loss',\n",
    "#                  metrics=['categorical_accuracy']\n",
    "                 metrics=['iou_score']\n",
    "                )\n",
    "    \n",
    "    history = model.fit_generator(train_generator,\n",
    "                                  steps_per_epoch = 367 // BATCH_SIZE,\n",
    "                                 epochs=epochs,\n",
    "                                 verbose=2,\n",
    "                                 validation_data=test_generator,\n",
    "                                 validation_steps=101 // BATCH_SIZE,\n",
    "                                 workers=22,\n",
    "                                 )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build UNet ...\n",
      "Epoch 1/20\n",
      " - 33s - loss: 1.6955 - iou_score: 0.0742 - val_loss: 1.4489 - val_iou_score: 0.0547\n",
      "Epoch 2/20\n",
      " - 20s - loss: 1.3970 - iou_score: 0.0849 - val_loss: 1.5228 - val_iou_score: 0.0866\n",
      "Epoch 3/20\n",
      " - 20s - loss: 1.6098 - iou_score: -1.1098e-02 - val_loss: 3.2107 - val_iou_score: 0.1174\n",
      "Epoch 4/20\n",
      " - 20s - loss: 2.1438 - iou_score: 0.0622 - val_loss: 1.3988 - val_iou_score: 0.0477\n",
      "Epoch 5/20\n",
      " - 20s - loss: 1.5156 - iou_score: 0.1041 - val_loss: 1.3296 - val_iou_score: 0.0938\n",
      "Epoch 6/20\n",
      " - 20s - loss: 2.6414 - iou_score: 0.1124 - val_loss: 1.6312 - val_iou_score: 0.0374\n",
      "Epoch 7/20\n",
      " - 20s - loss: 1.4024 - iou_score: 0.1048 - val_loss: 1.5894 - val_iou_score: 0.1185\n",
      "Epoch 8/20\n",
      " - 20s - loss: 1.8203 - iou_score: 0.0868 - val_loss: 1.3269 - val_iou_score: 0.1035\n",
      "Epoch 9/20\n",
      " - 20s - loss: 1.3459 - iou_score: 0.1019 - val_loss: 1.8945 - val_iou_score: 0.0500\n",
      "Epoch 10/20\n",
      " - 20s - loss: 1.5025 - iou_score: 0.0987 - val_loss: 1.3597 - val_iou_score: 0.0913\n",
      "Epoch 11/20\n",
      " - 20s - loss: 1.4017 - iou_score: 0.0985 - val_loss: 1.2804 - val_iou_score: 0.1212\n",
      "Epoch 12/20\n",
      " - 20s - loss: 1.5970 - iou_score: 0.1122 - val_loss: 1.5780 - val_iou_score: 0.0861\n",
      "Epoch 13/20\n",
      " - 20s - loss: 1.5788 - iou_score: 0.0917 - val_loss: 1.4392 - val_iou_score: 0.0886\n",
      "Epoch 14/20\n",
      " - 20s - loss: 1.3795 - iou_score: 0.0939 - val_loss: 1.3368 - val_iou_score: 0.1021\n",
      "Epoch 15/20\n",
      " - 20s - loss: 1.3294 - iou_score: 0.1006 - val_loss: 1.3172 - val_iou_score: 0.1105\n",
      "Epoch 16/20\n",
      " - 20s - loss: 1.2903 - iou_score: 0.1224 - val_loss: 1.4214 - val_iou_score: 0.1126\n",
      "Epoch 17/20\n",
      " - 20s - loss: 1.4763 - iou_score: 0.0997 - val_loss: 1.3765 - val_iou_score: 0.0682\n",
      "Epoch 18/20\n",
      " - 20s - loss: 1.4917 - iou_score: 0.0899 - val_loss: 1.3316 - val_iou_score: 0.1088\n",
      "Epoch 19/20\n"
     ]
    }
   ],
   "source": [
    "# Инициализация модели и запуск обучения\n",
    "model = init_model()\n",
    "model = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit_generator(train_generator,\n",
    "#                                   steps_per_epoch = 367 // BATCH_SIZE,\n",
    "#                                  epochs=epochs,\n",
    "#                                  verbose=2,\n",
    "#                                  validation_data=test_generator,\n",
    "#                                  validation_steps=101 // BATCH_SIZE,\n",
    "#                                  workers=22,\n",
    "#                                  )\n",
    "\n",
    "# model = init_model()\n",
    "# model.compile('Adam', \n",
    "#                  loss='categorical_crossentropy', \n",
    "#                  metrics=['categorical_accuracy'])\n",
    "\n",
    "# history = model.fit_generator(generator=train_generator,\n",
    "#                                 steps_per_epoch=len(train_ims) // BATCH_SIZE,\n",
    "#                                 epochs=200,\n",
    "#                                 verbose=2,\n",
    "#                                 workers=22)\n",
    "\n",
    "# predict = model.predict_generator(test_generator,\n",
    "#                        steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
